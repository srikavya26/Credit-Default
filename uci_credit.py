# -*- coding: utf-8 -*-
"""Uci credit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oi348UJ4DHNrHFcEznA0LNn8VkBqEZDA
"""



# ===========================
# 1. Import Required Libraries
# ===========================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')


from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                           f1_score, roc_auc_score, confusion_matrix,
                           classification_report, precision_recall_curve, roc_curve)
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB

# Advanced models
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTEENN

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks

plt.style.use('default')
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.titleweight'] = 'bold'

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ================================
# 2. DATA LOADING AND PREPROCESSING
# =================================
def load_credit_data(filepath='default of credit card clients.xls'):
    """
    Load the credit default dataset from Excel file or UCI repository

    Returns:
        pandas.DataFrame: Cleaned dataset with proper column names
    """
    print("="*60)
    print("DATA LOADING")
    print("="*60)

    try:
        df = pd.read_excel(filepath, sheet_name='Data', header=1)

        # Rename target column
        if 'default payment next month' in df.columns:
            df = df.rename(columns={'default payment next month': 'DEFAULT'})

        # Remove ID column if present
        if 'ID' in df.columns:
            df = df.drop(columns=['ID'])

        print(f" Dataset loaded successfully")
        print(f"  Shape: {df.shape[0]} samples, {df.shape[1]} features")
        print(f"  Default rate: {df['DEFAULT'].mean():.2%}")

        return df




def preprocess_data(df):
    """
    Perform basic data preprocessing and validation

    Args:
        df (pandas.DataFrame): Raw dataset

    Returns:
        pandas.DataFrame: Preprocessed dataset
    """
    print("\n" + "="*60)
    print("DATA PREPROCESSING")
    print("="*60)

    # Check for missing values
    missing_values = df.isnull().sum().sum()
    if missing_values > 0:
        print(f"⚠ Found {missing_values} missing values")
        # Simple imputation for numeric columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
    else:
        print(" No missing values found")

    # Check target variable
    if 'DEFAULT' not in df.columns:
        raise ValueError("Target column 'DEFAULT' not found in dataset")

    # Convert column names to uppercase for consistency
    df.columns = [col.upper() for col in df.columns]

    # Validate feature types
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    print(f" Numeric features: {len(numeric_features)}")

    return df

# ====================
# 3. DATA VISUALIZATION
# ====================
class DataVisualizer:

    @staticmethod
    def plot_class_distribution(y, title="Class Distribution"):

        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Get class counts
        counts = y.value_counts()
        class_names = ['Non-Default (0)', 'Default (1)']
        colors = ['#3498db', '#e74c3c']

        # Bar plot
        bars = axes[0].bar(class_names, counts, color=colors,
                          edgecolor='black', linewidth=1.5)
        axes[0].set_title('Class Distribution', fontsize=14, pad=15)
        axes[0].set_ylabel('Count')
        axes[0].grid(True, alpha=0.3, axis='y')

        # Add count labels
        for bar, count in zip(bars, counts):
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100,
                        f'{count:,}\n({count/len(y)*100:.1f}%)',
                        ha='center', fontsize=11, fontweight='bold')

        # Pie chart
        axes[1].pie(counts, labels=class_names, colors=colors,
                   autopct='%1.1f%%', startangle=90, explode=(0, 0.05),
                   shadow=True, textprops={'fontsize': 11})
        axes[1].set_title('Percentage Distribution', fontsize=14, pad=15)

        plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.show()

        # Print statistics
        imbalance_ratio = counts[0] / counts[1]
        print(f"Class imbalance ratio: {imbalance_ratio:.2f}:1")
        print(f"Default rate: {counts[1]/len(y)*100:.2f}%")

    @staticmethod
    def plot_feature_distributions(df, features=None, n_features=8):
        """
        Plot distributions of selected features

        Args:
            df (pandas.DataFrame): Dataset
            features (list): List of features to plot
            n_features (int): Number of features to plot if not specified
        """
        if features is None:
            # Select numeric features excluding target
            numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
            numeric_features = [f for f in numeric_features if f != 'DEFAULT']
            features = numeric_features[:n_features]

        n_rows = (len(features) + 3) // 4
        fig, axes = plt.subplots(n_rows, 4, figsize=(16, n_rows*3))
        axes = axes.flatten()

        for idx, feature in enumerate(features):
            if idx >= len(axes):
                break

            ax = axes[idx]
            ax.hist(df[feature], bins=30, alpha=0.7, color='steelblue',
                   edgecolor='black')
            ax.set_title(feature, fontsize=11)
            ax.set_xlabel('Value')
            ax.set_ylabel('Frequency')
            ax.grid(True, alpha=0.3)

            # Add statistics lines
            mean_val = df[feature].mean()
            median_val = df[feature].median()
            ax.axvline(mean_val, color='red', linestyle='--',
                      linewidth=1.5, label=f'Mean: {mean_val:.1f}')
            ax.axvline(median_val, color='green', linestyle=':',
                      linewidth=1.5, label=f'Median: {median_val:.1f}')
            ax.legend(fontsize=8)

        # Hide unused subplots
        for idx in range(len(features), len(axes)):
            axes[idx].axis('off')

        plt.suptitle('Feature Distributions', fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.show()

    @staticmethod
    def plot_correlation_analysis(df, target_col='DEFAULT'):
        """
        Analyze correlations between features and target

        Args:
            df (pandas.DataFrame): Dataset
            target_col (str): Target column name
        """
        # Calculate correlations with target
        correlations = df.corr()[target_col].sort_values(ascending=False)

        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        # Bar plot of top correlations
        top_n = min(15, len(correlations) - 1)
        top_features = correlations.iloc[1:top_n+1]  # Exclude target itself

        colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_features.values]
        axes[0].barh(range(len(top_features)), top_features.values, color=colors)
        axes[0].set_yticks(range(len(top_features)))
        axes[0].set_yticklabels(top_features.index, fontsize=10)
        axes[0].set_xlabel('Correlation with Default')
        axes[0].set_title(f'Top {top_n} Features Correlated with Default')
        axes[0].grid(True, alpha=0.3, axis='x')

        # Add correlation values
        for i, (feature, corr) in enumerate(zip(top_features.index, top_features.values)):
            axes[0].text(corr/2 if corr > 0 else corr*2, i,
                        f'{corr:.3f}', va='center',
                        ha='center' if corr > 0 else 'left',
                        fontweight='bold', color='white' if abs(corr) > 0.1 else 'black')

        # Scatter plot: Credit Limit vs Default (if available)
        if 'LIMIT_BAL' in df.columns:
            # Add jitter to y-values for better visualization
            y_jittered = df[target_col] + np.random.normal(0, 0.02, len(df))
            axes[1].scatter(df['LIMIT_BAL'], y_jittered,
                           alpha=0.3, s=20, color='purple')
            axes[1].set_xlabel('Credit Limit')
            axes[1].set_ylabel('Default (jittered)')
            axes[1].set_title('Credit Limit vs Default Probability')
            axes[1].grid(True, alpha=0.3)

        plt.suptitle('Feature Correlation Analysis', fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.show()

        return correlations

    @staticmethod
    def plot_payment_history_analysis(df, payment_cols=None):
        """
        Analyze payment history patterns

        Args:
            df (pandas.DataFrame): Dataset
            payment_cols (list): List of payment history column names
        """
        if payment_cols is None:
            # Find payment columns
            payment_cols = [col for col in df.columns if col.startswith('PAY_')]

        if not payment_cols:
            print("No payment history columns found")
            return

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()

        # 1. Default rate by payment status (most recent month)
        if 'PAY_0' in df.columns:
            pay_default = df.groupby('PAY_0')['DEFAULT'].agg(['count', 'mean'])
            pay_default = pay_default[pay_default['count'] > 50]  # Filter sparse groups

            axes[0].bar(pay_default.index.astype(str),
                       pay_default['mean'] * 100,
                       color='coral', alpha=0.7)
            axes[0].set_title('Default Rate by Most Recent Payment Status')
            axes[0].set_xlabel('Payment Status')
            axes[0].set_ylabel('Default Rate (%)')
            axes[0].grid(True, alpha=0.3, axis='y')

        # 2. Payment status trend over months
        axes[1].plot(range(len(payment_cols)), df[payment_cols].mean().values,
                    marker='o', linewidth=2, color='navy')
        axes[1].set_title('Average Payment Status Over Time')
        axes[1].set_xlabel('Month (0=Most Recent)')
        axes[1].set_ylabel('Average Payment Status')
        axes[1].grid(True, alpha=0.3)

        # 3. Credit limit analysis by default status
        if 'LIMIT_BAL' in df.columns:
            default_limits = df[df['DEFAULT'] == 1]['LIMIT_BAL']
            non_default_limits = df[df['DEFAULT'] == 0]['LIMIT_BAL']

            axes[2].boxplot([non_default_limits, default_limits],
                           labels=['Non-Default', 'Default'])
            axes[2].set_title('Credit Limit Distribution by Default Status')
            axes[2].set_ylabel('Credit Limit')
            axes[2].grid(True, alpha=0.3, axis='y')

        # 4. Age distribution by default status
        if 'AGE' in df.columns:
            for target, color, label in [(0, 'green', 'Non-Default'),
                                        (1, 'red', 'Default')]:
                subset = df[df['DEFAULT'] == target]['AGE']
                subset.plot(kind='kde', ax=axes[3], color=color,
                           label=label, linewidth=2)

            axes[3].set_title('Age Distribution by Default Status')
            axes[3].set_xlabel('Age')
            axes[3].set_ylabel('Density')
            axes[3].legend()
            axes[3].grid(True, alpha=0.3)

        plt.suptitle('Payment History and Demographic Analysis',
                    fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.show()

# ====================
# 4. MODEL IMPLEMENTATION
# ====================
class ModelImplementation:
    """Implements and evaluates machine learning models for credit default prediction"""

    def __init__(self, random_state=42):
        """
        Initialize model implementation

        Args:
            random_state (int): Random seed for reproducibility
        """
        self.random_state = random_state
        self.models = {}
        self.results = {}
        self.feature_names = None

    def create_baseline_models(self):
        """
        Create a set of baseline models for comparison

        Returns:
            dict: Dictionary of model names and initialized models
        """
        baseline_models = {
            'Logistic Regression': LogisticRegression(
                class_weight='balanced',
                max_iter=1000,
                random_state=self.random_state,
                solver='liblinear'
            ),
            'Decision Tree': DecisionTreeClassifier(
                max_depth=6,
                class_weight='balanced',
                random_state=self.random_state
            ),
            'Random Forest': RandomForestClassifier(
                n_estimators=100,
                class_weight='balanced',
                random_state=self.random_state,
                n_jobs=-1
            ),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=3,
                random_state=self.random_state
            ),
            'Naive Bayes': GaussianNB()
        }

        return baseline_models

    def create_xgboost_model(self, scale_pos_weight=None):
        """
        Create XGBoost model with optimized parameters

        Args:
            scale_pos_weight (float): Weight for positive class to handle imbalance

        Returns:
            XGBClassifier: Configured XGBoost model
        """
        if scale_pos_weight is None:

            scale_pos_weight = 3.5

        xgb_model = XGBClassifier(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=scale_pos_weight,
            reg_alpha=1,
            reg_lambda=3,
            eval_metric='logloss',
            random_state=self.random_state,
            n_jobs=-1,
            verbosity=0
        )

        return xgb_model

    def create_neural_network(self, input_dim):
        """
        Create a neural network model

        Args:
            input_dim (int): Number of input features

        Returns:
            keras.Model: Compiled neural network
        """
        model = keras.Sequential([
            layers.Input(shape=(input_dim,)),
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),

            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),

            layers.Dense(16, activation='relu'),

            layers.Dense(1, activation='sigmoid')
        ])

        # Compile model
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy',
                    keras.metrics.Precision(name='precision'),
                    keras.metrics.Recall(name='recall'),
                    keras.metrics.AUC(name='auc')]
        )

        return model

    def handle_class_imbalance(self, X_train, y_train, method='smote'):
        """
        Apply class imbalance handling techniques

        Args:
            X_train (array): Training features
            y_train (array): Training labels
            method (str): 'smote', 'smote_enn', or 'none'

        Returns:
            tuple: Resampled features and labels
        """
        if method == 'none':
            print("No imbalance handling applied")
            return X_train, y_train

        elif method == 'smote':
            smote = SMOTE(random_state=self.random_state)
            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
            print(f"SMOTE applied: {len(X_resampled)} samples")

        elif method == 'smote_enn':
            smote_enn = SMOTEENN(random_state=self.random_state)
            X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)
            print(f"SMOTE-ENN applied: {len(X_resampled)} samples")

        else:
            raise ValueError(f"Unknown method: {method}")

        print(f"  Before: Default rate = {y_train.mean():.2%}")
        print(f"  After:  Default rate = {y_resampled.mean():.2%}")

        return X_resampled, y_resampled

    def train_model(self, model, X_train, y_train, model_name):
        """
        Train a single model

        Args:
            model: Model object
            X_train (array): Training features
            y_train (array): Training labels
            model_name (str): Name of the model

        Returns:
            Trained model
        """
        print(f"Training {model_name}...")

        if isinstance(model, keras.Model):
            # Neural network training
            early_stopping = callbacks.EarlyStopping(
                monitor='loss',
                patience=5,
                restore_best_weights=True,
                verbose=0
            )

            model.fit(
                X_train, y_train,
                epochs=50,
                batch_size=32,
                callbacks=[early_stopping],
                verbose=0
            )
        else:
            # Scikit-learn model training
            model.fit(X_train, y_train)

        self.models[model_name] = model
        print(f"✓ {model_name} trained successfully")

        return model

    def evaluate_model(self, model, X_test, y_test, model_name, threshold=0.5):
        """
        Evaluate model performance

        Args:
            model: Trained model
            X_test (array): Test features
            y_test (array): Test labels
            model_name (str): Name of the model
            threshold (float): Classification threshold

        Returns:
            dict: Model evaluation metrics
        """
        print(f"\nEvaluating {model_name}:")

        # Get predictions
        if isinstance(model, keras.Model):
            y_pred_proba = model.predict(X_test, verbose=0).flatten()
        else:
            y_pred_proba = model.predict_proba(X_test)[:, 1]

        # Apply threshold
        y_pred = (y_pred_proba >= threshold).astype(int)

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        roc_auc = roc_auc_score(y_test, y_pred_proba)

        # Store results
        metrics = {
            'model_name': model_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }

        self.results[model_name] = metrics

        # Print results
        print(f"  Accuracy:  {accuracy:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall:    {recall:.4f}")
        print(f"  F1-Score:  {f1:.4f}")
        print(f"  ROC-AUC:   {roc_auc:.4f}")

        return metrics

    def cross_validate_model(self, model, X, y, model_name, cv_folds=5):
        """
        Perform cross-validation for robust evaluation

        Args:
            model: Model to evaluate
            X (array): Features
            y (array): Labels
            model_name (str): Name of the model
            cv_folds (int): Number of cross-validation folds

        Returns:
            dict: Cross-validation results
        """
        print(f"\nCross-validating {model_name} ({cv_folds}-fold)...")

        # Define scoring metrics
        scoring = {
            'accuracy': 'accuracy',
            'precision': 'precision',
            'recall': 'recall',
            'f1': 'f1',
            'roc_auc': 'roc_auc'
        }

        # Perform cross-validation
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True,
                           random_state=self.random_state)

        cv_results = {}
        for metric_name, metric_scorer in scoring.items():
            scores = cross_val_score(model, X, y, cv=cv,
                                   scoring=metric_scorer, n_jobs=-1)
            cv_results[metric_name] = {
                'mean': scores.mean(),
                'std': scores.std(),
                'scores': scores
            }

        # Print results
        print(f"{'Metric':<12} {'Mean':>8} {'Std':>8}")
        print("-" * 35)
        for metric, values in cv_results.items():
            print(f"{metric:<12} {values['mean']:>8.4f} {values['std']:>8.4f}")

        return cv_results

    def find_optimal_threshold(self, model, X_val, y_val, model_name):
        """
        Find optimal classification threshold based on F1-score

        Args:
            model: Trained model
            X_val (array): Validation features
            y_val (array): Validation labels
            model_name (str): Name of the model

        Returns:
            float: Optimal threshold
        """
        print(f"\nFinding optimal threshold for {model_name}...")

        # Get probability predictions
        if isinstance(model, keras.Model):
            y_pred_proba = model.predict(X_val, verbose=0).flatten()
        else:
            y_pred_proba = model.predict_proba(X_val)[:, 1]

        # Calculate precision-recall curve
        precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)

        # Find threshold that maximizes F1-score
        f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-10)
        optimal_idx = np.argmax(f1_scores)
        optimal_threshold = thresholds[optimal_idx]

        print(f"  Default threshold: 0.500")
        print(f"  Optimal threshold: {optimal_threshold:.3f}")
        print(f"  F1-Score at optimal: {f1_scores[optimal_idx]:.3f}")

        return optimal_threshold

    def get_results_summary(self):
        """
        Get summary of all model results

        Returns:
            pandas.DataFrame: Results summary
        """
        if not self.results:
            return pd.DataFrame()

        summary_data = []
        for model_name, metrics in self.results.items():
            summary_data.append({
                'Model': model_name,
                'Accuracy': metrics['accuracy'],
                'Precision': metrics['precision'],
                'Recall': metrics['recall'],
                'F1-Score': metrics['f1_score'],
                'ROC-AUC': metrics['roc_auc']
            })

        summary_df = pd.DataFrame(summary_data)
        summary_df = summary_df.sort_values('ROC-AUC', ascending=False)

        return summary_df

# ====================
# 5. RESULTS VISUALIZATION
# ====================
class ResultsVisualizer:
    """Visualizes model performance and comparison results"""

    @staticmethod
    def plot_model_comparison(results_dict):
        """
        Create bar chart comparing model performance

        Args:
            results_dict (dict): Dictionary of model results
        """
        models = list(results_dict.keys())
        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']

        # Prepare data
        comparison_data = []
        for model in models:
            for metric in metrics:
                if metric in results_dict[model]:
                    score = results_dict[model][metric]
                    comparison_data.append({
                        'Model': model,
                        'Metric': metric.replace('_', ' ').title(),
                        'Score': score
                    })

        df = pd.DataFrame(comparison_data)

        # Create grouped bar chart
        fig, ax = plt.subplots(figsize=(14, 8))

        # Set positions
        n_models = len(models)
        n_metrics = len(metrics)
        x = np.arange(n_models)
        width = 0.15

        # Colors for different metrics
        colors = plt.cm.Set1(np.linspace(0, 1, n_metrics))

        # Create bars for each metric
        for i, metric in enumerate(metrics):
            metric_scores = [results_dict[model].get(metric, 0) for model in models]
            ax.bar(x + i*width - (n_metrics-1)*width/2,
                  metric_scores, width, label=metric.replace('_', ' ').title(),
                  color=colors[i])

            # Add value labels
            for j, score in enumerate(metric_scores):
                ax.text(x[j] + i*width - (n_metrics-1)*width/2,
                       score + 0.01, f'{score:.3f}',
                       ha='center', va='bottom', fontsize=9, fontweight='bold')

        # Customize plot
        ax.set_xlabel('Models', fontsize=12)
        ax.set_ylabel('Score', fontsize=12)
        ax.set_title('Model Performance Comparison', fontsize=16, pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.set_ylim([0, 1.1])
        ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.show()

        return df

    @staticmethod
    def plot_confusion_matrices(results_dict, y_test, figsize=(16, 10)):
        """
        Plot confusion matrices for all models

        Args:
            results_dict (dict): Dictionary of model results
            y_test (array): True labels
            figsize (tuple): Figure size
        """
        n_models = len(results_dict)
        n_cols = min(3, n_models)
        n_rows = (n_models + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
        axes = axes.flatten() if n_models > 1 else [axes]

        for idx, (model_name, results) in enumerate(results_dict.items()):
            if idx >= len(axes):
                break

            ax = axes[idx]
            y_pred = results.get('y_pred', [])

            if len(y_pred) > 0:
                cm = confusion_matrix(y_test, y_pred)

                # Create heatmap
                im = ax.imshow(cm, interpolation='nearest', cmap='Blues')

                # Add text annotations
                thresh = cm.max() / 2
                for i in range(cm.shape[0]):
                    for j in range(cm.shape[1]):
                        ax.text(j, i, f'{cm[i, j]:,}\n({cm[i, j]/cm.sum()*100:.1f}%)',
                               ha="center", va="center",
                               color="white" if cm[i, j] > thresh else "black",
                               fontweight='bold')

                # Customize axes
                ax.set(xticks=[0, 1], yticks=[0, 1],
                      xticklabels=['Non-Default', 'Default'],
                      yticklabels=['Non-Default', 'Default'],
                      title=f'{model_name}\nAccuracy: {results["accuracy"]:.3f}',
                      ylabel='True Label',
                      xlabel='Predicted Label')

        # Hide unused subplots
        for idx in range(len(results_dict), len(axes)):
            axes[idx].axis('off')

        plt.suptitle('Confusion Matrices', fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.show()

    @staticmethod
    def plot_roc_curves(results_dict, y_test):
        """
        Plot ROC curves for all models

        Args:
            results_dict (dict): Dictionary of model results
            y_test (array): True labels
        """
        plt.figure(figsize=(10, 8))

        colors = plt.cm.Set1(np.linspace(0, 1, len(results_dict)))

        for (model_name, results), color in zip(results_dict.items(), colors):
            y_pred_proba = results.get('y_pred_proba', [])

            if len(y_pred_proba) > 0:
                fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
                roc_auc = results.get('roc_auc', roc_auc_score(y_test, y_pred_proba))

                plt.plot(fpr, tpr, color=color, lw=2,
                        label=f'{model_name} (AUC = {roc_auc:.3f})')

        # Plot random classifier line
        plt.plot([0, 1], [0, 1], 'k--', lw=1, label='Random Classifier')

        # Customize plot
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate (Recall)', fontsize=12)
        plt.title('Receiver Operating Characteristic (ROC) Curves',
                 fontsize=16, fontweight='bold', pad=20)
        plt.legend(loc='lower right')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    @staticmethod
    def plot_precision_recall_curves(results_dict, y_test, baseline_precision=None):
        """
        Plot Precision-Recall curves for all models

        Args:
            results_dict (dict): Dictionary of model results
            y_test (array): True labels
            baseline_precision (float): Baseline precision for random classifier
        """
        plt.figure(figsize=(10, 8))

        colors = plt.cm.Set1(np.linspace(0, 1, len(results_dict)))

        for (model_name, results), color in zip(results_dict.items(), colors):
            y_pred_proba = results.get('y_pred_proba', [])

            if len(y_pred_proba) > 0:
                precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)

                plt.plot(recall, precision, color=color, lw=2,
                        label=f'{model_name}')

        # Add baseline (random classifier)
        if baseline_precision:
            plt.axhline(y=baseline_precision, color='k', linestyle='--',
                       label=f'Baseline (Default Rate: {baseline_precision:.3f})')

        # Customize plot
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall', fontsize=12)
        plt.ylabel('Precision', fontsize=12)
        plt.title('Precision-Recall Curves', fontsize=16, fontweight='bold', pad=20)
        plt.legend(loc='upper right')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    @staticmethod
    def plot_feature_importance(model, feature_names, model_name="", top_n=15):
        """
        Plot feature importance for tree-based models

        Args:
            model: Trained model
            feature_names (list): List of feature names
            model_name (str): Name of the model
            top_n (int): Number of top features to show
        """
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importances = np.abs(model.coef_[0])
        else:
            print(f"Model {model_name} doesn't support feature importance")
            return

        # Sort features by importance
        indices = np.argsort(importances)[-top_n:]

        plt.figure(figsize=(12, 8))
        bars = plt.barh(range(top_n), importances[indices],
                       color='steelblue', edgecolor='black')

        plt.yticks(range(top_n), [feature_names[i] for i in indices])
        plt.xlabel('Importance Score', fontsize=12)
        plt.title(f'Top {top_n} Feature Importances - {model_name}',
                 fontsize=16, fontweight='bold', pad=20)
        plt.grid(True, alpha=0.3, axis='x')

        # Add value labels
        for i, (bar, importance) in enumerate(zip(bars, importances[indices])):
            plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
                    f'{importance:.4f}', va='center', fontsize=10, fontweight='bold')

        plt.tight_layout()
        plt.show()

# ====================
# 6. FEATURE ENGINEERING
# ====================
def create_features(X):
    """
    Create engineered features for better prediction

    Args:
        X (pandas.DataFrame): Original features

    Returns:
        pandas.DataFrame: Features with engineered columns
    """
    X = X.copy()

    print("\n" + "="*60)
    print("FEATURE ENGINEERING")
    print("="*60)

    original_feature_count = len(X.columns)

    # 1. Payment behavior features
    pay_cols = [col for col in X.columns if col.startswith('PAY_')]

    if len(pay_cols) >= 2:
        # Payment trend (worsening/improving over time)
        X['PAYMENT_TREND'] = X[pay_cols[0]] - X[pay_cols[-1]]

        # Payment volatility (standard deviation of payment status)
        X['PAYMENT_VOLATILITY'] = X[pay_cols].std(axis=1)

        # Count of late payments
        X['LATE_PAYMENTS_COUNT'] = (X[pay_cols] > 0).sum(axis=1)

        print("✓ Created payment behavior features")

    # 2. Credit utilization features
    if 'LIMIT_BAL' in X.columns:
        bill_cols = [col for col in X.columns if 'BILL_AMT' in col]

        if len(bill_cols) >= 2:
            # Current utilization ratio
            X['CURRENT_UTILIZATION'] = X[bill_cols[0]] / (X['LIMIT_BAL'] + 1)

            # Average utilization over time
            X['AVG_UTILIZATION'] = X[bill_cols].mean(axis=1) / (X['LIMIT_BAL'] + 1)

            # Utilization trend (increasing/decreasing)
            X['UTILIZATION_TREND'] = (X[bill_cols[0]] - X[bill_cols[-1]]) / (X['LIMIT_BAL'] + 1)

            print("✓ Created credit utilization features")

    # 3. Payment to bill ratios (ability/willingness to pay)
    for i in range(1, 7):
        bill_col = f'BILL_AMT{i}'
        pay_col = f'PAY_AMT{i}'

        if bill_col in X.columns and pay_col in X.columns:
            # Ensure denominator is not zero or negative to prevent inf/nan values
            denominator = X[bill_col] + 1
            X[f'PAYMENT_RATIO_{i}'] = np.where(
                denominator <= 0,
                0, # Assign 0 if denominator is problematic (e.g., bill_amt is -1 or less)
                X[pay_col] / denominator
            )

    # 4. Demographic risk encoding
    if 'AGE' in X.columns:
        # Create age groups
        age_bins = [20, 30, 40, 50, 60, 100]
        age_labels = ['20-29', '30-39', '40-49', '50-59', '60+']
        X['AGE_GROUP'] = pd.cut(X['AGE'], bins=age_bins, labels=age_labels)

        # Convert to numerical codes
        X['AGE_GROUP'] = X['AGE_GROUP'].cat.codes

        print("✓ Created demographic features")

    new_features = len(X.columns) - original_feature_count
    print(f"\nTotal new features created: {new_features}")
    print(f"Total features: {len(X.columns)}")

    return X

# ====================
# 7. MAIN EXECUTION PIPELINE
# ====================
def run_complete_analysis():
    """
    Run the complete credit default prediction analysis pipeline
    """
    print("="*70)
    print("CREDIT DEFAULT PREDICTION ANALYSIS")
    print("="*70)

    # Step 1: Load and preprocess data
    print("\n[1/7] LOADING DATA")
    df = load_credit_data()
    if df is None:
        print("Failed to load data. Exiting.")
        return

    df = preprocess_data(df)

    # Step 2: Exploratory Data Analysis
    print("\n[2/7] EXPLORATORY DATA ANALYSIS")
    visualizer = DataVisualizer()

    # Class distribution
    visualizer.plot_class_distribution(df['DEFAULT'])

    # Feature distributions
    visualizer.plot_feature_distributions(df)

    # Correlation analysis
    correlations = visualizer.plot_correlation_analysis(df)

    # Payment history analysis
    visualizer.plot_payment_history_analysis(df)

    # Step 3: Feature Engineering
    print("\n[3/7] FEATURE ENGINEERING")
    X = df.drop('DEFAULT', axis=1)
    y = df['DEFAULT']

    X_engineered = create_features(X)
    feature_names = X_engineered.columns.tolist()

    # Step 4: Data Preparation
    print("\n[4/7] DATA PREPARATION")

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X_engineered, y,
        test_size=0.2,
        stratify=y,
        random_state=42
    )

    # Create validation set from training data
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train,
        test_size=0.25,  # 20% of original data
        stratify=y_train,
        random_state=42
    )

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    print(f"Training set:   {X_train.shape[0]} samples")
    print(f"Validation set: {X_val.shape[0]} samples")
    print(f"Test set:       {X_test.shape[0]} samples")

    # Step 5: Model Implementation
    print("\n[5/7] MODEL IMPLEMENTATION")
    model_impl = ModelImplementation(random_state=42)

    # Handle class imbalance
    X_train_balanced, y_train_balanced = model_impl.handle_class_imbalance(
        X_train_scaled, y_train, method='smote'
    )

    # Create and train baseline models
    print("\nTraining baseline models...")
    baseline_models = model_impl.create_baseline_models()

    for name, model in baseline_models.items():
        trained_model = model_impl.train_model(model, X_train_balanced, y_train_balanced, name)
        model_impl.evaluate_model(trained_model, X_test_scaled, y_test, name)

    # Train XGBoost
    print("\nTraining XGBoost...")
    xgb_model = model_impl.create_xgboost_model()
    trained_xgb = model_impl.train_model(xgb_model, X_train_balanced, y_train_balanced, "XGBoost")
    model_impl.evaluate_model(trained_xgb, X_test_scaled, y_test, "XGBoost")

    # Train Neural Network
    print("\nTraining Neural Network...")
    nn_model = model_impl.create_neural_network(X_train_balanced.shape[1])
    trained_nn = model_impl.train_model(nn_model, X_train_balanced, y_train_balanced, "Neural Network")
    model_impl.evaluate_model(trained_nn, X_test_scaled, y_test, "Neural Network")

    # Step 6: Model Evaluation
    print("\n[6/7] MODEL EVALUATION")

    # Get results summary
    results_summary = model_impl.get_results_summary()

    if not results_summary.empty:
        print("\nModel Performance Summary:")
        print("="*60)
        print(results_summary.to_string(index=False))

        # Find best model
        best_model_name = results_summary.iloc[0]['Model']
        print(f"\nBest performing model: {best_model_name}")
        print(f"ROC-AUC: {results_summary.iloc[0]['ROC-AUC']:.4f}")

    # Step 7: Results Visualization
    print("\n[7/7] RESULTS VISUALIZATION")
    results_viz = ResultsVisualizer()

    # Model comparison
    if model_impl.results:
        results_viz.plot_model_comparison(model_impl.results)
        results_viz.plot_confusion_matrices(model_impl.results, y_test)
        results_viz.plot_roc_curves(model_impl.results, y_test)
        results_viz.plot_precision_recall_curves(model_impl.results, y_test, y.mean())

        # Feature importance for best tree-based model
        if 'XGBoost' in model_impl.models:
            results_viz.plot_feature_importance(
                model_impl.models['XGBoost'],
                feature_names,
                model_name="XGBoost"
            )

    print("\n" + "="*70)
    print("ANALYSIS COMPLETED SUCCESSFULLY")

run_complete_analysis()